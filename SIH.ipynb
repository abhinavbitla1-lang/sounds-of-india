{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78703dd6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dc659f17",
   "metadata": {},
   "source": [
    "# Developing a software for dubbing of videos from English to other Indian regional languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7fd7e",
   "metadata": {},
   "source": [
    "# converting mp4 to mp3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1445fd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter video path : nasdaily.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='audio.wav'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# conda install -c conda-forge ffmpeg\n",
    "# video to wav audio\n",
    "from os import path\n",
    "from pydub import AudioSegment\n",
    "import pydub\n",
    "\n",
    "# files\n",
    "src = input(\"Enter video path : \")\n",
    "dst = \"audio.wav\"\n",
    "\n",
    "# convert mp4 to wav\n",
    "#pydub.AudioSegment.ffmpeg = \"C:/Users/prasa/sem 5 projects/ffmpeg-6.0\"\n",
    "#target_bitrate = \"128k\"\n",
    "sound = AudioSegment.from_file(src,format=\"mp4\")#bitrate=target_bitrate\n",
    "sound.export(dst, format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa229f0",
   "metadata": {},
   "source": [
    "pip install numpy==1.18.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5094133d",
   "metadata": {},
   "source": [
    "pip install googletrans"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269e1e91",
   "metadata": {},
   "source": [
    "pip install moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4bbcd",
   "metadata": {},
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6570a503",
   "metadata": {},
   "source": [
    "pip install gtts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622ebe78",
   "metadata": {},
   "source": [
    "pip install googletrans==4.0.0-rc1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5662a26c",
   "metadata": {},
   "source": [
    "pip install translate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f23a57",
   "metadata": {},
   "source": [
    "pip install pysrt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88eed52e",
   "metadata": {},
   "source": [
    "pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059fa1ea",
   "metadata": {},
   "source": [
    "pip install spleeter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdb4d76",
   "metadata": {},
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ef06ba8",
   "metadata": {},
   "source": [
    "pip install pydub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0bc0994",
   "metadata": {},
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00c50dcd",
   "metadata": {},
   "source": [
    "pip install SpeechRecognition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dc3c80",
   "metadata": {},
   "source": [
    "pip install pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df9e4f1",
   "metadata": {},
   "source": [
    "pip install --upgrade httpx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c8f97b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3f7e744",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "print(np.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a933b270",
   "metadata": {},
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f6d66e",
   "metadata": {},
   "source": [
    "# Dividing music and audio from audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da0c9242",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Using config: {'_model_dir': 'pretrained_models\\\\2stems', '_tf_random_seed': None, '_save_summary_steps': 100, '_save_checkpoints_steps': None, '_save_checkpoints_secs': 600, '_session_config': gpu_options {\n",
      "  per_process_gpu_memory_fraction: 0.7\n",
      "}\n",
      ", '_keep_checkpoint_max': 5, '_keep_checkpoint_every_n_hours': 10000, '_log_step_count_steps': 100, '_train_distribute': None, '_device_fn': None, '_protocol': None, '_eval_distribute': None, '_experimental_distribute': None, '_experimental_max_worker_delay_secs': None, '_session_creation_timeout_secs': 7200, '_checkpoint_save_graph_def': True, '_service': None, '_cluster_spec': ClusterSpec({}), '_task_type': 'worker', '_task_id': 0, '_global_id_in_cluster': 0, '_master': '', '_evaluation_master': '', '_is_chief': True, '_num_ps_replicas': 0, '_num_worker_replicas': 1}\n",
      "WARNING:tensorflow:From C:\\Users\\prasa\\.conda\\envs\\py38\\lib\\site-packages\\spleeter\\separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_types is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "WARNING:tensorflow:From C:\\Users\\prasa\\.conda\\envs\\py38\\lib\\site-packages\\spleeter\\separator.py:146: calling DatasetV2.from_generator (from tensorflow.python.data.ops.dataset_ops) with output_shapes is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use output_signature instead\n",
      "INFO:tensorflow:Calling model_fn.\n",
      "INFO:tensorflow:Apply unet for vocals_spectrogram\n",
      "WARNING:tensorflow:From C:\\Users\\prasa\\.conda\\envs\\py38\\lib\\site-packages\\keras\\layers\\normalization\\batch_normalization.py:514: _colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "INFO:tensorflow:Apply unet for accompaniment_spectrogram\n",
      "INFO:tensorflow:Done calling model_fn.\n",
      "INFO:tensorflow:Graph was finalized.\n",
      "INFO:tensorflow:Restoring parameters from pretrained_models\\2stems\\model\n",
      "INFO:tensorflow:Running local_init_op.\n",
      "INFO:tensorflow:Done running local_init_op.\n"
     ]
    }
   ],
   "source": [
    "from spleeter.separator import Separator\n",
    "\n",
    "# Initialize Spleeter with the 2stems model\n",
    "separator = Separator('spleeter:2stems')\n",
    "\n",
    "# Provide the input audio file path\n",
    "audio_file = \"C:/Users/prasa/OneDrive/Desktop/audio.mp3\"\n",
    "\n",
    "# Separate music and vocals\n",
    "separator.separate_to_file(audio_file, \"C:/Users/prasa/OneDrive/Desktop/Both\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c37ec98",
   "metadata": {},
   "source": [
    "# OR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e978e45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf4d61c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7b7eacc1",
   "metadata": {},
   "source": [
    "# Converting image to text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed634b3",
   "metadata": {},
   "source": [
    "###### 0    Orientation and script detection (OSD) only.\n",
    "###### 1    Automatic page segmentation with OSD.\n",
    "###### 2    Automatic page segmentation, but no OSD, or OCR.\n",
    "###### 3    Fully automatic page segmentation, but no OSD. (Default)\n",
    "###### 4    Assume a single column of text of variable sizes.\n",
    "###### 5    Assume a single uniform block of vertically aligned text.\n",
    "###### 6    Assume a single uniform block of text.\n",
    "###### 7    Treat the image as a single text line.\n",
    "###### 8    Treat the image as a single word.\n",
    "###### 9    Treat the image as a single word in a circle.\n",
    "###### 10    Treat the image as a single character.\n",
    "###### 11    Sparse text. Find as much text as possible in no particular order.\n",
    "###### 12    Sparse text with OSD.\n",
    "###### 13    Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54945f94",
   "metadata": {},
   "source": [
    "## Page segmentation modes:\n",
    "\n",
    "###### Orientation and script detection (OSD) only.\n",
    "###### Automatic page segmentation with OSD.\n",
    "###### Automatic page segmentation, but no OSD, or OCR. (not implemented)\n",
    "###### Fully automatic page segmentation, but no OSD. (Default) \n",
    "###### Assume a single column of text of variable sizes.\n",
    "###### Assume a single uniform block of vertically aligned text.\n",
    "###### Assume a single uniform block of text.\n",
    "###### Treat the image as a single text line.\n",
    "###### Treat the image as a single word.\n",
    "###### Treat the image as a single word in a circle.\n",
    "###### Treat the image as a single character.\n",
    "###### Sparse text. Find as much text as possible in no particular order.\n",
    "###### Sparse text with OSD.\n",
    "###### Raw line. Treat the image as a single text line, bypassing hacks that are Tesseract-specific.\n",
    "\n",
    "## OCR Engine modes:\n",
    "\n",
    "###### Legacy engine only.\n",
    "###### Neural nets LSTM engine only.\n",
    "###### Legacy + LSTM engines.\n",
    "###### Default, based on what is available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ed58efa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter Path:nasdaily.mp4\n",
      "0\n",
      "Detected Text:\n",
      "v&\n",
      "\n",
      "y]\n",
      "\n",
      "Thisjis\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "_\n",
      "\n",
      "vf\n",
      "\n",
      "Thisjis\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "-\n",
      "\n",
      "4,\n",
      "\n",
      "Tins is\n",
      "\n",
      "thefonly\n",
      "\n",
      "Detected Text:\n",
      "mel\n",
      "\n",
      "4 2\n",
      "\n",
      "sab\n",
      "\n",
      "Thisjis\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "maf\n",
      "\n",
      "Tins is\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "ins is\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "~\n",
      "\n",
      "-~,\n",
      "\n",
      "Tins is\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "Tihisjis\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "Tjhisjis\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "‘ Tihisjis\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "¢ Whisyis\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "-\n",
      "\n",
      "y\n",
      "\n",
      "4 Thisyis\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "~\n",
      "\n",
      "y\n",
      "\n",
      "« Tihis\\is\n",
      "\n",
      "the/only\n",
      "\n",
      "Detected Text:\n",
      "é Tihisjis\n",
      "\n",
      "the/fonly\n",
      "\n",
      "Detected Text:\n",
      "4 Thisjis\n",
      "\n",
      "theyonly\n",
      "\n",
      "Detected Text:\n",
      "é Thisjis\n",
      "\n",
      "theyonly\n",
      "\n",
      "Detected Text:\n",
      "a\n",
      "\n",
      "d “Thisjis\n",
      "\n",
      "theyonly\n",
      "\n",
      "Detected Text:\n",
      "fe\n",
      "\n",
      "&\n",
      "\n",
      "\"This is\n",
      "\n",
      "theyonly\n",
      "\n",
      "Detected Text:\n",
      "$e\n",
      "\n",
      "Thisjis\n",
      "\n",
      "theyonly\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import pytesseract\n",
    "import time\n",
    "\n",
    "# Path to the Tesseract executable (update this path based on your installation)\n",
    "pytesseract.pytesseract.tesseract_cmd = r'E:\\tesseract\\tesseract.exe'\n",
    "\n",
    "# Create a video capture object (0 for the default camera, or specify a video file)\n",
    "cap = cv2.VideoCapture(input(\"Enter Path:\"))\n",
    "x=[]\n",
    "print(len(x))\n",
    "while True:\n",
    "    # Read a frame from the video\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Convert the frame to grayscale (optional)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # Apply any preprocessing here (e.g., noise reduction, contrast adjustment)\n",
    "\n",
    "    # Use pytesseract to detect text in the frame\n",
    "    myconfig = r\"--psm 11 --oem 3\"\n",
    "    text = pytesseract.image_to_string(frame,config = myconfig)\n",
    "\n",
    "    # Display the frame with detected text\n",
    "    \n",
    "    #cv2.imshow('Video Text Detection', frame)\n",
    "\n",
    "    # Print the detected text (you can save it to a file or perform other actions)\n",
    "    print('Detected Text:')\n",
    "    if len(x) == 0:\n",
    "        x.append(text)\n",
    "        print(text)\n",
    "    else:\n",
    "        if x[len(x)-1] != text:\n",
    "            x.append(text)\n",
    "            print(text)\n",
    "        else:\n",
    "            continue\n",
    "    #time.sleep(2)\n",
    "    # Break the loop if the 'q' key is pressed\n",
    "    #if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "    #    break\n",
    "\n",
    "# Release the video capture object and close all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eacf3e75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1bfe0212",
   "metadata": {},
   "source": [
    "# Converting Audio to text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b99e4c15",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative Text: this is the only animal in the world that was once gone and is now back to life let me tell you the incredible story of the golden horse the Mongolian horse is a truly Wild Horse but in the 1960s it went extinct real world got together to save the Mongolian horse scientists carefully read them until they win from 12:00 to 50 to 100 when the world comes together with one goal we can achieve anything we can fix the ozone layer we can make peace and we can bring animals\n",
      "Alternative Text: this is the only animal in the world that was once gone and is now back to life let me tell you the incredible story of the golden horse the Mongolian horse is a truly Wild Horse butt in the 1960s it went extinct real world got together to save the Mongolian horse scientists carefully read them until they win from 12:00 to 50 to 100 when the world comes together with one goal we can achieve anything we can fix the ozone layer we can make peace and we can bring animals\n",
      "Alternative Text: this is the only animal in the world that was once gone and is now back to life let me tell you the incredible story of the golden horse the Mongolian horse is a truly Wild Horse but in the 1960s it went extinct real world got together to save the Mongolian horse scientists carefully read them until they win from 12:00 to 50 to 100 when the world comes together with one goal we can achieve anything we can fix the ozone layer we can make piece and we can bring animals\n",
      "Alternative Text: this is the only animal in the world that was once gone and is now back to life let me tell you the incredible story of the golden horse the Mongolian horse is a truly Wild Horse butt in the 1960s it went extinct real world got together to save the Mongolian horse scientists carefully read them until they win from 12:00 to 50 to 100 when the world comes together with one goal we can achieve anything we can fix the ozone layer we can make piece and we can bring animals\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "l = []\n",
    "def recognize_audio(audio_file):\n",
    "    r = sr.Recognizer()\n",
    "\n",
    "    try:\n",
    "        with sr.AudioFile(audio_file) as source:\n",
    "            audio = r.record(source)\n",
    "            text = r.recognize_google(audio, language=\"en-\", show_all=True)\n",
    "\n",
    "            if text:\n",
    "                if \"alternative\" in text:\n",
    "                    alternatives = text[\"alternative\"]\n",
    "                    for alt in alternatives:\n",
    "                        print(\"Alternative Text:\", alt[\"transcript\"])\n",
    "                        l.append(alt[\"transcript\"])\n",
    "                else:\n",
    "                    print(\"Recognized Text:\", text)\n",
    "\n",
    "            else:\n",
    "                print(\"No text recognized.\")\n",
    "\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Could not understand the audio.\")\n",
    "    except sr.RequestError as e:\n",
    "        print(f\"Could not request results: {e}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file = \"Both/audio/vocals.wav\"\n",
    "    recognize_audio(audio_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64ad3d1",
   "metadata": {},
   "source": [
    "from translate import Translator\n",
    "def text_trans(text,o_lang):\n",
    "    english_text = text\n",
    "    \n",
    "    # Create a Translator object\n",
    "    translator = Translator(to_lang=o_lang)  # \"te\" is the language code for Telugu\n",
    "    \n",
    "    # Translate the text\n",
    "    translated_text = translator.translate(english_text)\n",
    "    return translated_text\n",
    "    # Print the translated text\n",
    "    #print(translated_text)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "930cbc2e",
   "metadata": {},
   "source": [
    "# Translating Eng text to desired language "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288caf9",
   "metadata": {},
   "source": [
    "from googletrans import Translator\n",
    "def text_image(text,o_lang):\n",
    "    translator = Translator(service_urls='translate.googleapis.com')\n",
    "    english_text = text\n",
    "    translated_text = translator.translate(english_text, dest=o_lang)\n",
    "    \n",
    "    #print(f\"English: {english_text}\")\n",
    "    print(f\"{translated_text.src}: {translated_text.origin}\")\n",
    "    print(f\"\\n{translated_text.dest}: {translated_text.text}\")\n",
    "    return translated_text.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ada7f97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from googletrans import Translator\n",
    "def text_image(text,o_lang):\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(text, dest=o_lang)\n",
    "    print(f\"Original Text: Hello, how are you?\")\n",
    "    print(f\"Translated Text ({translated_text.src}): {translated_text.text}\")\n",
    "    translated_text = translated_text.text\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a52573c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "af: afrikaans\n",
      "sq: albanian\n",
      "am: amharic\n",
      "ar: arabic\n",
      "hy: armenian\n",
      "az: azerbaijani\n",
      "eu: basque\n",
      "be: belarusian\n",
      "bn: bengali\n",
      "bs: bosnian\n",
      "bg: bulgarian\n",
      "ca: catalan\n",
      "ceb: cebuano\n",
      "ny: chichewa\n",
      "zh-cn: chinese (simplified)\n",
      "zh-tw: chinese (traditional)\n",
      "co: corsican\n",
      "hr: croatian\n",
      "cs: czech\n",
      "da: danish\n",
      "nl: dutch\n",
      "en: english\n",
      "eo: esperanto\n",
      "et: estonian\n",
      "tl: filipino\n",
      "fi: finnish\n",
      "fr: french\n",
      "fy: frisian\n",
      "gl: galician\n",
      "ka: georgian\n",
      "de: german\n",
      "el: greek\n",
      "gu: gujarati\n",
      "ht: haitian creole\n",
      "ha: hausa\n",
      "haw: hawaiian\n",
      "iw: hebrew\n",
      "he: hebrew\n",
      "hi: hindi\n",
      "hmn: hmong\n",
      "hu: hungarian\n",
      "is: icelandic\n",
      "ig: igbo\n",
      "id: indonesian\n",
      "ga: irish\n",
      "it: italian\n",
      "ja: japanese\n",
      "jw: javanese\n",
      "kn: kannada\n",
      "kk: kazakh\n",
      "km: khmer\n",
      "ko: korean\n",
      "ku: kurdish (kurmanji)\n",
      "ky: kyrgyz\n",
      "lo: lao\n",
      "la: latin\n",
      "lv: latvian\n",
      "lt: lithuanian\n",
      "lb: luxembourgish\n",
      "mk: macedonian\n",
      "mg: malagasy\n",
      "ms: malay\n",
      "ml: malayalam\n",
      "mt: maltese\n",
      "mi: maori\n",
      "mr: marathi\n",
      "mn: mongolian\n",
      "my: myanmar (burmese)\n",
      "ne: nepali\n",
      "no: norwegian\n",
      "or: odia\n",
      "ps: pashto\n",
      "fa: persian\n",
      "pl: polish\n",
      "pt: portuguese\n",
      "pa: punjabi\n",
      "ro: romanian\n",
      "ru: russian\n",
      "sm: samoan\n",
      "gd: scots gaelic\n",
      "sr: serbian\n",
      "st: sesotho\n",
      "sn: shona\n",
      "sd: sindhi\n",
      "si: sinhala\n",
      "sk: slovak\n",
      "sl: slovenian\n",
      "so: somali\n",
      "es: spanish\n",
      "su: sundanese\n",
      "sw: swahili\n",
      "sv: swedish\n",
      "tg: tajik\n",
      "ta: tamil\n",
      "te: telugu\n",
      "th: thai\n",
      "tr: turkish\n",
      "uk: ukrainian\n",
      "ur: urdu\n",
      "ug: uyghur\n",
      "uz: uzbek\n",
      "vi: vietnamese\n",
      "cy: welsh\n",
      "xh: xhosa\n",
      "yi: yiddish\n",
      "yo: yoruba\n",
      "zu: zulu\n"
     ]
    }
   ],
   "source": [
    "from googletrans import LANGUAGES\n",
    "\n",
    "for code, language in LANGUAGES.items():\n",
    "    print(f\"{code}: {language}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1572b8f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter language : te\n",
      "Original Text: Hello, how are you?\n",
      "Translated Text (en): ప్రపంచంలోని ఏకైక జంతువు ఇది ఒకప్పుడు పోయింది మరియు ఇప్పుడు తిరిగి జీవితానికి తిరిగి వచ్చింది, గోల్డెన్ హార్స్ యొక్క నమ్మశక్యం కాని కథ మంగోలియన్ గుర్రం నిజంగా అడవి గుర్రం, కానీ 1960 లలో ఇది అంతరించిపోయిన వాస్తవ ప్రపంచం కలిసిపోయిందిమంగోలియన్ గుర్రపు శాస్త్రవేత్తలు 12:00 నుండి 50 నుండి 100 వరకు గెలిచే వరకు వాటిని జాగ్రత్తగా చదివారు, ప్రపంచం ఒక లక్ష్యంతో కలిసి వచ్చినప్పుడు మనం ఏదైనా సాధించగలము, ఓజోన్ పొరను పరిష్కరించగల ఏదైనా మనం శాంతిని కలిగించవచ్చు మరియు మేము జంతువులను తీసుకురావచ్చు\n"
     ]
    }
   ],
   "source": [
    "o_lang = input(\"Enter language : \")\n",
    "translated_text = text_image(l[0],o_lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67603cb3",
   "metadata": {},
   "source": [
    "from googletrans import Translator\n",
    "def text_image(text,o_lang):\n",
    "    translator = Translator()\n",
    "    translated_text = translator.translate(text, dest=o_lang)\n",
    "    print(f\"Original Text: Hello, how are you?\")\n",
    "    print(f\"Translated Text ({translated_text.src}): {translated_text.text}\")\n",
    "    translated_text = translated_text.text\n",
    "    return translated_text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c323f7f",
   "metadata": {},
   "source": [
    "# Converting text to audio named as translated_text.wav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "444442e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.8.18)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "from io import BytesIO\n",
    "import pygame\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4bdb740f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text converted to speech and saved as 'translated_text.wav'\n"
     ]
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "tts = gTTS(translated_text)\n",
    "\n",
    "# Save the speech to a WAV file\n",
    "tts.save(\"translated_text.wav\")\n",
    "\n",
    "print(\"Text converted to speech and saved as 'translated_text.wav'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e9d07d",
   "metadata": {},
   "source": [
    "# Comparing original audio and trasnlated text audio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2566c9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9141a36",
   "metadata": {},
   "source": [
    "# Adding Translated text and music andnamed as merged_audio.wav"
   ]
  },
  {
   "cell_type": "raw",
   "id": "500bafa3",
   "metadata": {},
   "source": [
    "import sounddevice as sd\n",
    "import numpy as np\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "existing_audio = AudioSegment.from_wav('Music_audio.wav')\n",
    "# Start recording live audio\n",
    "duration = len(existing_audio)/1000  # Adjust the duration as needed\n",
    "live_audio = sd.rec(int(44100 * duration), samplerate=44100, channels=2, dtype='int16')\n",
    "sd.wait()\n",
    "\n",
    "# Convert the live audio NumPy array to an AudioSegment\n",
    "live_audio = live_audio.tobytes()\n",
    "\n",
    "# Mix the live audio with the existing audio\n",
    "mixed_audio = existing_audio.overlay(AudioSegment(live_audio, sample_width=2, frame_rate=44100, channels=2))\n",
    "\n",
    "# Export the mixed audio to a new WAV file\n",
    "mixed_audio.export('mixed_audio.wav', format='wav')\n",
    "\n",
    "# Play the mixed audio\n",
    "# play(mixed_audio)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1616dc",
   "metadata": {},
   "source": [
    "# Saving only Video from the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c8ea9b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Extraction done\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture('nasdaily.mp4')\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(\"Only_video.mp4\", fourcc, fps, (width,height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    out.write(frame)\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Frames Extraction done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1508f3cb",
   "metadata": {},
   "source": [
    "# Decreasing the music sound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b30329b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='Both/audio/accompaniment.wav'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load the audio file\n",
    "audio = AudioSegment.from_file(\"Both/audio/accompaniment.wav\")\n",
    "\n",
    "# Define the amount by which you want to decrease the volume in dB\n",
    "volume_reduction_dB = 25  # Adjust this value as needed\n",
    "\n",
    "# Reduce the volume\n",
    "audio = audio - volume_reduction_dB\n",
    "\n",
    "# Export the modified audio to a new file\n",
    "audio.export(\"Both/audio/accompaniment.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "711b03e8",
   "metadata": {},
   "source": [
    "# Decreasing translated_text.wav sound by 25%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f5c19db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='Both/audio/translated_text.wav'>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load the audio file\n",
    "audio = AudioSegment.from_file(\"translated_text.wav\")\n",
    "\n",
    "# Define the amount by which you want to decrease the volume in dB\n",
    "volume_reduction_dB = 15  # Adjust this value as needed\n",
    "\n",
    "# Reduce the volume\n",
    "audio = audio - volume_reduction_dB\n",
    "\n",
    "# Export the modified audio to a new file\n",
    "audio.export(\"Both/audio/translated_text.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3677ab0",
   "metadata": {},
   "source": [
    "# Equalision both Audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5d5c6cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='Both/audio/translated_text_new.wav'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load the original audio file (47 seconds)\n",
    "audio = AudioSegment.from_file(\"Both/audio/translated_text.wav\")\n",
    "\n",
    "# Calculate the duration of the audio file in milliseconds\n",
    "original_duration_ms = len(audio)\n",
    "\n",
    "# Define the target duration in milliseconds (60 seconds)\n",
    "target_duration_ms = 60 * 1000\n",
    "\n",
    "# Calculate the duration difference\n",
    "duration_difference_ms = target_duration_ms - original_duration_ms\n",
    "\n",
    "# Create a silent audio segment with the desired duration difference\n",
    "silence = AudioSegment.silent(duration=duration_difference_ms)\n",
    "\n",
    "# Concatenate the original audio with the silent audio to extend the duration\n",
    "extended_audio = audio + silence\n",
    "\n",
    "# Export the extended audio to a new file\n",
    "extended_audio.export(\"Both/audio/translated_text_new.wav\", format=\"wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67fcb901",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "46108c2c",
   "metadata": {},
   "source": [
    "# Merging audios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05afab14",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='Both/audio/merged_audio.wav'>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# Load the audio files\n",
    "audio1 = AudioSegment.from_file(\"Both/audio/accompaniment.wav\")\n",
    "audio2 = AudioSegment.from_file(\"Both/audio/translated_text_new.wav\")\n",
    "\n",
    "# Compare the durations of the two audio files\n",
    "if len(audio2) < len(audio1):\n",
    "    # Calculate the difference in duration\n",
    "    duration_diff = len(audio1) - len(audio2)\n",
    "\n",
    "    # Extend the shorter audio to match the duration of the longer one\n",
    "    audio2 = audio2 + AudioSegment.silent(duration=duration_diff)\n",
    "\n",
    "# Merge the two audio files\n",
    "merged_audio = audio1.overlay(audio2)\n",
    "\n",
    "# Export the merged audio\n",
    "merged_audio.export(\"Both/audio/merged_audio.wav\", format=\"wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bf2051c",
   "metadata": {},
   "source": [
    "# Merging audios and videos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e2be86",
   "metadata": {},
   "source": [
    "pip install moviepy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e432057",
   "metadata": {},
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip\n",
    "\n",
    "# Load the video and audio files\n",
    "video_clip = VideoFileClip(\"Only_video.mp4\")\n",
    "audio_clip = AudioFileClip(\"Both/audio/merged_audio.wav\")\n",
    "video = cv2.VideoCapture(\"Only_video.mp4\")\n",
    "fps = int(video.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Set the audio of the video to the loaded audio\n",
    "video_clip = video_clip.set_audio(audio_clip)\n",
    "\n",
    "# Write the merged video with the added audio to a new file\n",
    "video_clip.write_videofile(\"Both/audio/final_video.mp4\", codec=\"libx264\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fa3e90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb7e76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f3c1ba1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "129f6b12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ffmpeg\n",
    "\n",
    "# Define the paths to your audio and video files\n",
    "video_file_path = 'Only_video.mp4'\n",
    "audio_file_path = 'Both/audio/merged_audio.wav'\n",
    "\n",
    "# Define the output file path for the merged video\n",
    "output_file_path = 'Both/audio/final_video.mp4'\n",
    "\n",
    "# Use ffmpeg-python to merge audio and video\n",
    "ffmpeg.input(video_file_path).output(output_file_path, vf='pad=ceil(iw/2)*2:ceil(ih/2)*2', af=audio_file_path).run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e82bae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Muxing Done\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "cmd = 'ffmpeg -y -i Both/audio/merged_audio.wav  -r 30 -i Only_video.mp4  -filter:a aresample=async=1 -c:a flac -c:v copy Both/audio/final_video.mp4'\n",
    "subprocess.call(cmd, shell=True)                                     # \"Muxing Done\n",
    "print('Muxing Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bef11a5",
   "metadata": {},
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip, concatenate_videoclips\n",
    "import os\n",
    "# Get the desired video title\n",
    "# Open the video and audio\n",
    "video_clip = VideoFileClip(\"Only_video.mp4\")\n",
    "audio_clip = AudioFileClip(\"Both/audio/merged_audio.wav\")\n",
    "# Concatenate the video clip with the audio clip\n",
    "final_clip = video_clip.set_audio(audio_clip)\n",
    "final_clip.write_videofile(\"Both/audio/final_video.mp4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fe2405",
   "metadata": {},
   "source": [
    "# converting to srt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bc68d05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "00fc1b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Both/audio/audio.srt\",\"w\") as f:\n",
    "            f.write(l[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2c5fde16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is the only animal in the world that was once gone and is now back to life let me tell you the incredible story of the golden horse the Mongolian horse is a truly Wild Horse but in the 1960s it went extinct we lost all Mongolian Wild Horses got together to save the Mongolian horse scientists carefully read them until they win from 12:00 to 50 to 100 when the world comes together with one goal we can achieve anything we can fix the ozone layer we can make peace and we can bring animals\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import pysrt\n",
    "\n",
    "def audio_to_text(audio_file):\n",
    "    recognizer = sr.Recognizer()\n",
    "\n",
    "    with sr.AudioFile(audio_file) as source:\n",
    "        audio_data = recognizer.record(source)\n",
    "        text = recognizer.recognize_google(audio_data)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def text_to_srt(audio_file_path, text, output_srt):\n",
    "    subs = pysrt.SubRipFile()\n",
    "    sub = pysrt.SubRipItem()\n",
    "    sub.index = 1\n",
    "    sub.start = pysrt.SubRipTime(0, 0, 0, 0)\n",
    "    sub.end = pysrt.SubRipTime(0, 0, len(audio_file_path))  # Set the duration as needed\n",
    "    sub.text = text\n",
    "    subs.append(sub)\n",
    "\n",
    "    subs.save(output_srt, encoding='utf-8')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    audio_file_path = \"Both/audio/vocals.wav\"\n",
    "    output_srt_path = \"Both/audio/vocals_srt.srt\"\n",
    "\n",
    "    text = text = l[0]\n",
    "    print(text)\n",
    "    text_to_srt(audio_file_path, text, output_srt_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec265b53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "07a21e30",
   "metadata": {},
   "source": [
    "# adding subtitle file to the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb024ea",
   "metadata": {},
   "source": [
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from moviepy.video.compositing.CompositeVideoClip import CompositeVideoClip\n",
    "\n",
    "def add_subtitle(video_path, srt_path, output_path):\n",
    "    # Load video clip\n",
    "    video_clip = VideoFileClip(video_path)\n",
    "    \n",
    "    # Set a dummy duration for the SRT file (adjust as needed)\n",
    "    dummy_duration = video_clip.duration\n",
    "    \n",
    "    # Load SRT file\n",
    "    subtitles = VideoFileClip(srt_path, duration=dummy_duration).subtitles\n",
    "    \n",
    "    # Add subtitles to video\n",
    "    video_clip = CompositeVideoClip([video_clip, subtitles.set_pos(('center', 'bottom'))])\n",
    "    \n",
    "    # Write the result to a new file\n",
    "    video_clip.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\", temp_audiofile=\"temp-audio.m4a\", remove_temp=True)\n",
    "\n",
    "# Example usage\n",
    "video_path = \"Both/audio/final_video.mp4\"\n",
    "srt_path = \"Both/audio/vocals_srt.srt\"M\n",
    "output_path = \"Both/audio/audio_subtitles.mp4\"\n",
    "add_subtitle(video_path, srt_path, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e778e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba460586",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4653cc9d",
   "metadata": {},
   "source": [
    "# Removing directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de0b4df0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'Both/' does not exist.\n"
     ]
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        if (window._pyforest_update_imports_cell) { window._pyforest_update_imports_cell('import os'); }\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory 'translated_text.wav' does not exist.\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "\n",
    "# Specify the directory path you want to remove\n",
    "directory_path = \"Both/\"\n",
    "directory_path1 = \"translated_text.wav\"\n",
    "\n",
    "# Check if the directory exists\n",
    "if os.path.exists(directory_path) and os.path.isdir(directory_path):\n",
    "    try:\n",
    "        # Remove the directory and its contents\n",
    "        shutil.rmtree(directory_path)\n",
    "        print(f\"Directory '{directory_path}' and its contents removed successfully.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path}' does not exist.\")\n",
    "\n",
    "if os.path.exists(directory_path1) and os.path.isdir(directory_path1):\n",
    "    try:\n",
    "        # Remove the directory and its contents\n",
    "        shutil.rmtree(directory_path1)\n",
    "        print(f\"Directory '{directory_path1}' and its contents removed successfully.\")\n",
    "    except OSError as e:\n",
    "        print(f\"Error: {e}\")\n",
    "else:\n",
    "    print(f\"Directory '{directory_path1}' does not exist.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17258e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gui\n",
    "\n",
    "from tkinter import *\n",
    "from tkinter import filedialog\n",
    "import imageio_ffmpeg as ff  # Assuming this is the library for video processing\n",
    "\n",
    "# Function to handle video compression based on the selected codec\n",
    "def compress_video():\n",
    "    global vid\n",
    "    codec_choice = codec_var.get()  # Get the selected codec from the variable\n",
    "    if vid:\n",
    "        if codec_choice == 1:\n",
    "            libx264(vid)\n",
    "        elif codec_choice == 2:\n",
    "            libx265(vid)\n",
    "        elif codec_choice == 3:\n",
    "            libvpx_vp9(vid)\n",
    "        elif codec_choice == 4:\n",
    "            mpeg2video(vid)\n",
    "        elif codec_choice == 5:\n",
    "            libaom_av1(vid)\n",
    "        elif codec_choice == 6:\n",
    "            ffv1(vid)\n",
    "        elif codec_choice == 7:\n",
    "            huffyuv(vid)\n",
    "        elif codec_choice == 8:\n",
    "            utvideo(vid)\n",
    "        elif codec_choice == 9:\n",
    "            lagarith(vid)\n",
    "        else:\n",
    "            print(\"Invalid Choice\")\n",
    "    else:\n",
    "        print(\"No video file selected.\")\n",
    "\n",
    "# Function to handle file browsing\n",
    "def browse_files():\n",
    "    global vid\n",
    "    filename = filedialog.askopenfilename(initialdir=\"/\", title=\"Select a File\",\n",
    "                                          filetypes=((\"Video files\", \"*.mp4 *.avi *.webm *.mkv *.mpg\"), (\"all files\", \"*.*\")))\n",
    "    if filename:\n",
    "        vid = filename\n",
    "        label_file_explorer.config(text=\"File Opened: \" + filename)\n",
    "    else:\n",
    "        label_file_explorer.config(text=\"No file selected.\")\n",
    "\n",
    "# Tkinter GUI setup\n",
    "window = Tk()\n",
    "window.title('Video Codec Selector')\n",
    "window.geometry(\"500x200\")\n",
    "\n",
    "# Variable to store the selected video file\n",
    "vid = None\n",
    "\n",
    "# Label to display file explorer\n",
    "label_file_explorer = Label(window, text=\"No file selected.\", width=50, height=2, fg=\"blue\")\n",
    "label_file_explorer.pack()\n",
    "\n",
    "# Button to browse files\n",
    "button_explore = Button(window, text=\"Browse Files\", command=browse_files)\n",
    "button_explore.pack()\n",
    "\n",
    "# Dropdown menu for codec selection\n",
    "codec_var = IntVar()\n",
    "codec_var.set(1)  # Default codec choice\n",
    "codec_choices = [\n",
    "    (\"libx264\", 1),\n",
    "    (\"libx265\", 2),\n",
    "    (\"libvpx-vp9\", 3),\n",
    "    (\"mpeg2video\", 4),\n",
    "    (\"libaom-av1\", 5),\n",
    "    (\"ffv1\", 6),\n",
    "    (\"huffyuv\", 7),\n",
    "    (\"utvideo\", 8),\n",
    "    (\"lagarith\", 9)\n",
    "]\n",
    "\n",
    "# Create radio buttons for codec selection\n",
    "for codec, choice in codec_choices:\n",
    "    Radiobutton(window, text=codec, variable=codec_var, value=choice).pack()\n",
    "\n",
    "# Button to compress the video\n",
    "compress_button = Button(window, text=\"Compress Video\", command=compress_video)\n",
    "compress_button.pack()\n",
    "\n",
    "window.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b7c78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d6cf1e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frames Extraction done\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "cap = cv2.VideoCapture('sih folder/video.mp4')\n",
    "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "out = cv2.VideoWriter(\"sih folder/Only_video.mp4\", fourcc, fps, (width,height))\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    out.write(frame)\n",
    "cap.release()\n",
    "out.release()\n",
    "print(\"Frames Extraction done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c36debe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merging auido and video..........\n",
      "Mixing Done\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "print(\"Merging auido and video..........\")\n",
    "cmd = 'ffmpeg -y -i sih_folder/audio.wav  -r 30 -i sih_folder/Only_video.mp4  -filter:a aresample=async=1 -c:a flac -c:v copy sih_folder/final_video.mp4'\n",
    "subprocess.call(cmd, shell=True)                                     # \"Muxing Done\n",
    "print('Mixing Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43299cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
